{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{},"source":["# Sales_(oasis_1) - Cleaned Notebook\n","This notebook was reconstructed from the provided content and saved in a valid JSON structure. It contains the main import, data loading, preprocessing, EDA and cleaning steps.\n"]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# Import libraries\n","import pandas as pd\n","import numpy as np\n","import re\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from collections import Counter\n","import nltk\n","from nltk.tokenize import word_tokenize\n","import warnings\n","warnings.filterwarnings('ignore')\n","nltk.download('punkt')\n"],"outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# Load datasets (update paths if needed)\n","conversation_df = pd.read_csv('/Conversation.csv')\n","quotes_df = pd.read_csv('/train (1).csv')\n","print('Conversation shape:', conversation_df.shape)\n","print('Quotes shape:', quotes_df.shape)\n"],"outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# Basic preprocessing helpers\n","def preprocess_text(text):\n","    if pd.isna(text):\n","        return ''\n","    text = str(text).lower()\n","    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n","    text = re.sub(r'\\s+', ' ', text).strip()\n","    return text\n","\n","# Combine datasets into a single list and save training_data.txt\n","all_text = []\n","all_text.extend(conversation_df['question'].dropna().astype(str).tolist())\n","all_text.extend(conversation_df['answer'].dropna().astype(str).tolist())\n","all_text.extend(quotes_df.iloc[:,0].dropna().astype(str).tolist())\n","processed_text = [preprocess_text(t) for t in all_text if t]\n","processed_text = [t for t in processed_text if len(t) > 0]\n","with open('training_data.txt', 'w', encoding='utf-8') as f:\n","    for t in processed_text:\n","        f.write(t + '\\n')\n","print('Saved training_data.txt with', len(processed_text), 'lines')\n"],"outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# Simple EDA - top words\n","sample_texts = processed_text[:1000]\n","all_words = []\n","for txt in sample_texts:\n","    all_words.extend(word_tokenize(txt))\n","word_freq = Counter(all_words)\n","most_common = word_freq.most_common(20)\n","print('Top 20 words:', most_common)\n"],"outputs":[]},{"cell_type":"code","metadata":{},"execution_count":null,"source":["# Combine into a dataframe for cleaning\n","temp_quotes = quotes_df.copy()\n","temp_quotes['question'] = np.nan\n","temp_quotes['answer'] = np.nan\n","temp_conv = conversation_df.copy()\n","if 'Unnamed: 0' in temp_conv.columns:\n","    temp_conv = temp_conv.drop(columns=['Unnamed: 0'])\n","temp_conv['Quotes'] = np.nan\n","cols = ['Quotes','question','answer']\n","temp_quotes = temp_quotes[cols]\n","temp_conv = temp_conv[cols]\n","df = pd.concat([temp_quotes, temp_conv], ignore_index=True)\n","# Normalize columns\n","for c in ['Quotes','question','answer']:\n","    df[c] = df[c].astype(str).apply(preprocess_text)\n","# Save combined dataframe for later use\n","df.to_csv('combined_data_cleaned.csv', index=False, encoding='utf-8')\n","print('Saved combined_data_cleaned.csv with', len(df), 'rows')\n"],"outputs":[]},{"cell_type":"markdown","metadata":{},"source":["Notes:\n","- This notebook is a cleaned, valid JSON reconstruction focusing on the code cells from the provided content.\n","- If you want the full original notebook (with all markdown and outputs) restored exactly, please provide the complete raw .ipynb JSON file (it appears the original was truncated/invalid).\n"]}],"language_info":{"name":"python"}}