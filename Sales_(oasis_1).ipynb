{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"provenance": []}, "kernelspec": {"name": "python3", "display_name": "Python 3"}, "language_info": {"name": "python"}, "widgets": {"application/vnd.jupyter.widget-state+json": {"71c2498537e44a36bf733b45d4c0739b": {"model_module": "@jupyter-widgets/controls", "model_name": "HBoxModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_e8dba302dc75434898075ad3280c8b22", "IPY_MODEL_82b08cb32b13486f97d02d5a234b2fe1", "IPY_MODEL_3d21e9b22e434a3a9548cbe10356c500"], "layout": "IPY_MODEL_0b2e05e1d9ba46e3b506936f2b64c636"}}, "e8dba302dc75434898075ad3280c8b22": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_6155dc3646f142d8b470d4e3b885c547", "placeholder": "\u200b", "style": "IPY_MODEL_be01e199edb54b3eb55491161df01d84", "value": "tokenizer_config.json:\u2027100%"}}, "82b08cb32b13486f97d02d5a234b2fe1": {"model_module": "@jupyter-widgets/controls", "model_name": "FloatProgressModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_cad476a6d95f4e78bc832032dcb51b3d", "max": 26, "min": 0, "orientation": "horizontal", "style": "IPY_MODEL_cc85cc24074d4788b65da78b56be1d4e", "value": 26}}, "3d21e9b22e434a3a9548cbe10356c500": {"model_module": "@jupyter-widgets/controls", "model_name": "HTMLModel", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_c7e3c50993d841e69cef4c6de3637af7", "placeholder": "\u200b", "style": "IPY_MODEL_fe50f96ec7b94123b99488fde0850435", "value": "\u200b26.0/26.0\u200b[00:00<00:00,\u200b590B/s]"}}, "0b2e05e1d9ba46e3b506936f2b64c636": {"model_module": "@jupyter-widgets/base", "model_name": "LayoutModel", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}}}, "cells": [{"cell_type": "code", "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "8e1abcde", "outputId": "f369d2c7-4da0-43fb-9d04-0584ef6d19c4"}, "source": ["#Import libraries\\n", "import pandas as pd\\n", "import numpy as np\\n", "import re\\n", "import string\\n", "from collections import Counter, defaultdict\\n", "import matplotlib.pyplot as plt\\n", "import seaborn as sns\\n", "from tqdm import tqdm\\n", "import torch\\n", "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling\\n", "from transformers import Trainer, TrainingArguments\\n", "import nltk\\n", "from nltk.corpus import words\\n", "from nltk.tokenize import word_tokenize\\n", "import warnings\\n", "warnings.filterwarnings('ignore')\\n", "\\n", "# Download NLTK data\\n", "nltk.download('punkt')\\n", "nltk.download('words')\\n", "nltk.download('averaged_perceptron_tagger')\\n", "\\n", "print(\"✓ All libraries imported successfully!\")\n"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["# **STEP 2: LOAD AND EXPLORE DATASETS**"], "metadata": {"id": "uYmOyITN-aUQ"}}, {"cell_type": "code", "source": ["conversation_df = pd.read_csv('/Conversation.csv')\\n", "quotes_df = pd.read_csv('/train (1).csv')\\n", "\\n", "print(\"Dataset Information:\\")\\n", "print(f\"\\nConversation Dataset Shape: {conversation_df.shape}\")\\n", "print(conversation_df.head())\\n", "print(f\"\\nQuotes Dataset Shape: {quotes_df.shape}\")\\n", "print(quotes_df.head())\\n", "\\n", "# Combine all text data\\n", "all_text = []\\n", "all_text.extend(conversation_df['question'].dropna().tolist())\\n", "all_text.extend(conversation_df['answer'].dropna().tolist())\\n", "all_text.extend(quotes_df['Quotes'].dropna().tolist())\\n", "\\n", "print(f\"\\nTotal text samples: {len(all_text)}\")\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "yOlIanMONL3q", "outputId": "63c5b16f-38b4-4243-840b-594941ca6284"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["# **STEP 3: DATA PREPROCESSING**"], "metadata": {"id": "RDKYkMJV_JQz"}}, {"cell_type": "code", "source": ["def preprocess_text(text):\\n", "    \"\"\"Clean and preprocess text data\"\"\"\\n", "    if pd.isna(text):\\n", "        return \"\"\\n", "    text = str(text).lower()\\n", "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\\n", "    text = re.sub(r'\\s+', ' ', text)\\n", "    text = text.strip()\\n", "    return text\\n", "\\n", "# Preprocess all text\\n", "processed_text = [preprocess_text(text) for text in all_text if text]\\n", "processed_text = [text for text in processed_text if len(text) > 10]\\n", "\\n", "print(f\"Processed text samples: {len(processed_text)}\")\\n", "print(f\"Sample: {processed_text[0][:100]}\")\\n", "\\n", "# Create a combined text file for training\\n", "with open('training_data.txt', 'w', encoding='utf-8') as f:\\n", "    for text in processed_text:\\n", "        f.write(text + '\\n')\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "AbVi4kco_Jtm", "outputId": "1bb3ffa5-ea61-4389-fb29-19eae57f5beb"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["# **STEP 4: EXPLORATORY DATA ANALYSIS**"], "metadata": {"id": "uuAqTB4N_Wdo"}}, {"cell_type": "code", "source": ["nltk.download('punkt_tab')\\n", "\\n", "# Tokenize and analyze\\n", "all_words = []\\n", "for text in processed_text[:1000]:  # Sample for faster analysis\\n", "    words = word_tokenize(text)\\n", "    all_words.extend(words)\\n", "\\n", "# Word frequency analysis\\n", "word_freq = Counter(all_words)\\n", "most_common = word_freq.most_common(20)\\n", "\\n", "# Visualization\\n", "plt.figure(figsize=(12, 6))\\n", "words, counts = zip(*most_common)\\n", "plt.bar(words, counts, color='steelblue')\\n", "plt.xlabel('Words')\\n", "plt.ylabel('Frequency')\\n", "plt.title('Top 20 Most Common Words')\\n", "plt.xticks(rotation=45, ha='right')\\n", "plt.tight_layout()\\n", "plt.show()\\n", "\\n", "# Text length distribution\\n", "text_lengths = [len(text.split()) for text in processed_text]\\n", "plt.figure(figsize=(10, 5))\\n", "plt.hist(text_lengths, bins=50, color='coral', edgecolor='black')\\n", "plt.xlabel('Number of Words')\\n", "plt.ylabel('Frequency')\\n", "plt.title('Distribution of Text Lengths')\\n", "plt.show()\\n", "\\n", "print(f\"Average text length: {np.mean(text_lengths):.2f} words\")\\n", "print(f\"Vocabulary size: {len(word_freq)} unique words\")\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "id": "Ftcffnx3_YbB", "outputId": "90472998-011e-4983-aedd-2e051723210a"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["# **4.CLEANING PROCESS**"], "metadata": {"id": "Sz3WmZ86MygB"}}, {"cell_type": "code", "source": ["# Create a temporary DataFrame for quotes, adding 'question' and 'answer' columns with NaN\\n", "temp_quotes = quotes_df.copy()\\n", "temp_quotes['question'] = np.nan\\n", "temp_quotes['answer'] = np.nan\\n", "\\n", "# Create a temporary DataFrame for conversations, adding a 'Quotes' column with NaN\\n", "temp_conversation = conversation_df.copy()\\n", "temp_conversation['Quotes'] = np.nan\\n", "# Drop the 'Unnamed: 0' column if it exists in conversation_df to avoid issues during concat\\n", "if 'Unnamed: 0' in temp_conversation.columns:\\n", "    temp_conversation = temp_conversation.drop(columns=['Unnamed: 0'])\\n", "\\n", "# Ensure both temporary DataFrames have the same columns before concatenation\\n", "cols = ['Quotes', 'question', 'answer']\\n", "temp_quotes = temp_quotes[cols]\\n", "temp_conversation = temp_conversation[cols]\\n", "\\n", "# Concatenate the two DataFrames to create the main 'df'\\n", "df = pd.concat([temp_quotes, temp_conversation], ignore_index=True)\\n", "\\n", "print(\"Combined DataFrame 'df' created successfully.\")\\n", "print(df.head())\\n", "print(df.tail())\\n", "print(df.info())\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "87-hpLA3M6vg", "outputId": "5d91721a-07d3-4b20-b768-de12fb81c9ea"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["import re\\n", "\\n", "def fix_encoding(text):\\n", "    if isinstance(text, str):\\n", "        # A basic approach to handle potential encoding issues by re-encoding and decoding\\n", "        # This might not fix all issues but can help with common artifacts.\\n", "        try:\\n", "            return text.encode('latin-1', 'ignore').decode('utf-8', 'ignore')\\n", "        except (UnicodeEncodeError, UnicodeDecodeError):\\n", "            return text # Return original if problematic\\n", "    return ''\\n", "\\n", "def normalize_text(text):\\n", "    if isinstance(text, str):\\n", "        # Remove extra spaces and strip leading/trailing whitespace\\n", "        text = re.sub(r'\\s+', ' ', text).strip()\\n", "        return text\\n", "    return ''\\n", "\\n", "# Apply fix_encoding (this might not show visible changes if encoding was already fine)\\n", "df['Quotes_Fixed_Encoding'] = df['Quotes_No_Numbers'].apply(fix_encoding)\\n", "df['question_Fixed_Encoding'] = df['question_No_Numbers'].apply(fix_encoding)\\n", "df['answer_Fixed_Encoding'] = df['answer_No_Numbers'].apply(fix_encoding)\\n", "\\n", "# Apply normalize (remove extra spaces)\\n", "df['Quotes_Normalized'] = df['Quotes_Fixed_Encoding'].apply(normalize_text)\\n", "df['question_Normalized'] = df['question_Fixed_Encoding'].apply(normalize_text)\\n", "df['answer_Normalized'] = df['answer_Fixed_Encoding'].apply(normalize_text)\\n", "\\n", "print(\"Original (No Numbers) and Fixed Encoding 'Quotes' column:\\")\\n", "display(df[['Quotes_No_Numbers', 'Quotes_Fixed_Encoding']].head())\\n", "\\n", "print(\"\\nFixed Encoding and Normalized (No Extra Spaces) 'Quotes' column:\\")\\n", "display(df[['Quotes_Fixed_Encoding', 'Quotes_Normalized']].head())\\n", "\\n", "print(\"\\nOriginal (No Numbers) and Fixed Encoding and Normalized 'question' and 'answer' columns (for conversation entries):\" )\\n", "display(df[df['question'].notna()][['question_No_Numbers', 'question_Fixed_Encoding', 'question_Normalized', 'answer_No_Numbers', 'answer_Fixed_Encoding', 'answer_Normalized']].head())\n"], "metadata": {"id": "ZEA3jYmFpJsi", "colab": {"base_uri": "https://localhost:8080/", "height": 777}, "outputId": "eb6c6472-3d66-4e4f-fe01-0fef77052323"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def remove_duplicate_sentences(sentences):\\n", "    if isinstance(sentences, list):\\n", "        return list(dict.fromkeys(sentences)) # Preserves order\\n", "    return []\\n", "\\n", "df['Quotes_Sentences_No_Duplicates'] = df['Quotes_Sentences'].apply(remove_duplicate_sentences)\\n", "df['question_Sentences_No_Duplicates'] = df['question_Sentences'].apply(remove_duplicate_sentences)\\n", "df['answer_Sentences_No_Duplicates'] = df['answer_Sentences'].apply(remove_duplicate_sentences)\\n", "\\n", "print(\"Sentences after removing duplicates for 'Quotes':\")\\n", "display(df[['Quotes_Sentences', 'Quotes_Sentences_No_Duplicates']].head())\\n", "\\n", "print(\"\\nSentences after removing duplicates for 'question' and 'answer':\")\\n", "display(df[df['question'].notna()][['question_Sentences', 'question_Sentences_No_Duplicates', 'answer_Sentences', 'answer_Sentences_No_Duplicates']].head())\n"], "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["import nltk\\n", "from nltk.tokenize import sent_tokenize\\n", "nltk.download('punkt', quiet=True)\\n", "print(\"NLTK punkt tokenizer downloaded and imported.\")\n"], "metadata": {"id": "c3754d5b", "colab": {"base_uri": "https://localhost:8080/", "height": 577}, "outputId": "71cb3c2d-f6bc-4b80-c1ad-643543be7a92"}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": ["def split_into_sentences(text):\\n", "    if isinstance(text, str):\\n", "        return sent_tokenize(text)\\n", "    return []\\n", "\\n", "# Apply sentence splitting\\n", "df['Quotes_Sentences'] = df['Quotes_Normalized'].apply(split_into_sentences)\\n", "df['question_Sentences'] = df['question_Normalized'].apply(split_into_sentences)\\n", "df['answer_Sentences'] = df['answer_Normalized'].apply(split_into_sentences)\\n", "\\n", "print(\"Sentences extracted for 'Quotes_Normalized':\")\\n", "display(df[['Quotes_Normalized', 'Quotes_Sentences']].head())\\n", "\\n", "print(\"\\nSentences extracted for 'question_Normalized' and 'answer_Normalized':\")\\n", "display(df[df['question'].notna()][['question_Normalized', 'question_Sentences', 'answer_Normalized', 'answer_Sentences']].head())\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 482}, "id": "40f33e84", "outputId": "06684dfc-770d-429c-e20e-2d81a660f96b"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["# **STEP 5: BUILD N-GRAM BASED AUTOCOMPLETE (BASELINE)**"], "metadata": {"id": "jqpq3BN1CHSk"}}, {"cell_type": "code", "source": ["class NgramAutocomplete:\\n", "    def __init__(self, n=3):\\n", "        self.n = n\\n", "        self.ngrams = defaultdict(Counter)\\n", "\\n", "    def train(self, texts):\\n", "        \"\"\"Train n-gram model\"\"\"\\n", "        for text in tqdm(texts, desc=\"Training N-gram model\"):\\n", "            words = text.split()\\n", "            for i in range(len(words) - self.n + 1):\\n", "                context = tuple(words[i:i+self.n-1])\\n", "                next_word = words[i+self.n-1]\\n", "                self.ngrams[context][next_word] += 1\\n", "\\n", "    def predict(self, context, top_k=5):\\n", "        \"\"\"Predict next words given context\"\"\"\\n", "        context_words = context.lower().split()\\n", "        context_tuple = tuple(context_words[-(self.n-1):])\\n", "\\n", "        if context_tuple in self.ngrams:\\n", "            predictions = self.ngrams[context_tuple].most_common(top_k)\\n", "            return [word for word, _ in predictions]\\n", "        return []\\n", "\\n", "# Train baseline model\\n", "print(\"\\n\" + \"="*50)\\n", "print(\"TRAINING BASELINE N-GRAM MODEL\")\\n", "print(\"="*50)\\n", "ngram_model = NgramAutocomplete(n=3)\\n", "ngram_model.train(processed_text)\\n", "print(\"✓ N-gram model trained!\")\\n", "\\n", "# Test baseline\\n", "test_context = \"how are\"\\n", "predictions = ngram_model.predict(test_context)\\n", "print(f\"\\nContext: '{test_context}'\")\\n", "print(f\"Predictions: {predictions}\")\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "9NeFnpj2CDwj", "outputId": "2d5cbb18-aebd-4cf2-8743-908693891f87"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["# **STEP 6: BUILD AUTOCORRECT SYSTEM**"], "metadata": {"id": "7ZxHEHqaCXVR"}}, {"cell_type": "code", "source": ["class SpellCorrector:\\n", "    def __init__(self):\\n", "        self.word_list = set(nltk.corpus.words.words()) # Corrected to use nltk.corpus.words\\n", "        self.word_freq = Counter()\\n", "\\n", "    def train(self, texts):\\n", "        \"\"\"Build frequency dictionary\"\"\"\\n", "        for text in tqdm(texts, desc=\"Training spell corrector\"):\\n", "            words_in_text = word_tokenize(text.lower())\\n", "            self.word_freq.update(words_in_text)\\n", "\\n", "    def edit_distance_1(self, word):\\n", "        \"\"\"Generate all words one edit away\"\"\"\\n", "        letters = string.ascii_lowercase\\n", "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\\n", "        deletes = [L + R[1:] for L, R in splits if R]\\n", "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\\n", "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\\n", "        inserts = [L + c + R for L, R in splits for c in letters]\\n", "        return set(deletes + transposes + replaces + inserts)\\n", "\\n", "    def known_words(self, words):\\n", "        \"\"\"Filter to known words\"\"\"\\n", "        return set(w for w in words if w in self.word_freq)\\n", "\\n", "    def correct(self, word):\\n", "        \"\"\"Get best correction for word\"\"\"\\n", "        if word in self.word_freq:\\n", "            return word\\n", "\\n", "        candidates = self.known_words(self.edit_distance_1(word))\\n", "        if not candidates:\\n", "            candidates = self.known_words([w2 for w1 in self.edit_distance_1(word)\\n", "                                          for w2 in self.edit_distance_1(w1)])\\n", "        if not candidates:\\n", "            return word\\n", "\\n", "        return max(candidates, key=self.word_freq.get)\\n", "\\n", "# Train spell corrector\\n", "print(\"\\n\" + \"="*50)\\n", "print(\"TRAINING SPELL CORRECTOR\")\\n", "print(\"="*50)\\n", "spell_corrector = SpellCorrector()\\n", "spell_corrector.train(processed_text)\\n", "print(\"✓ Spell corrector trained!\")\\n", "\\n", "# Test spell corrector\\n", "test_words = [\"helo\", \"wrld\", \"progrm\", \"machne\"]\\n", "print(\"\\nSpell Correction Examples:\\")\\n", "for word in test_words:\\n", "    correction = spell_corrector.correct(word)\\n", "    print(f\"  {word} → {correction}\")\n"], "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "uJlaVeonCicK", "outputId": "6dcabd53-8fa5-409b-e881-31b8f5c4256b"}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": ["# **STEP 7: FINE-TUNE GPT-2 FOR ADVANCED AUTOCOMPLETE**"], "metadata": {"id": "08HssIVrCxG3"}}, {"cell_type": "code", "source": ["print(\"\\n\" + \"="*50)\\n", "print(\"FINE-TUNING GPT-2 MODEL\")\\n", "print(\"="*50)\\n", "\\n", "# Load pre-trained GPT-2\\n", "model_name = 'gpt2'\\n", "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\\n", "tokenizer.pad_token = tokenizer.eos_token\\n", "model = GPT2LMHeadModel.from_pretrained(model_name)\\n", "\\n", "# Prepare dataset\\n", "def load_dataset(file_path, tokenizer, block_size=128):\\n", "    return TextDataset(\\n", "        tokenizer=tokenizer,\\n", "        file_path=file_path,\\n", "        block_size=block_size\\n", "    )\\n", "\\n", "train_dataset = load_dataset('training_data.txt', tokenizer)\\n", "data_collator = DataCollatorForLanguageModeling(\\n", "    tokenizer=tokenizer,\\n", "    mlm=False\\n", ")\\n", "\\n", "# Training arguments (adjust for your needs)\\n", "training_args = TrainingArguments(\\n", "    output_dir='./results',\\n", "    overwrite_output_dir=True,\\n", "    num_train_epochs=3,\\n", "    per_device_train_batch_size=4,\\n", "    save_steps=500,\\n", "    save_total_limit=2,\\n", "    logging_steps=100,\\n", "    learning_rate=5e-5,\\n", "    warmup_steps=100,\\n", ")\\n", "\\n", "# Initialize trainer\\n", "trainer = Trainer(\\n", "    model=model,\\n", "    args=training_args,\\n", "    data_collator=data_collator,\\n", "    train_dataset=train_dataset,\\n", "]}